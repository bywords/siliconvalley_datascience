---
title: "Chapter 7"
author: "Kunwoo Park"
date: '2017 11 23 '
output:
  pdf_document:
    latex_engine: xelatex
mainfont: NanumGothic
---

## 7.1 데이터형, 분석 기법, R 함수

* 80%의 실제 문제는 20% 정도의 통계 기법으로 처리할 수 있다.

```{r}
library(ggplot2)
library(dplyr)
mpg <- tbl_df(mpg)
mpg
```

## 7.2 모든 데이터에 행해야 할 분석

* 데이터 내용, 구조, 타입 파악: dplyr::glimpse()
* 데이터 요약 통계량 파악: summary()
* 결측치 파악: summary()
* 시각화: plot(), pairs(), dplyr::sample_n()

```{r}
glimpse(mpg)
head(mpg)
summary(mpg)
```

## 7.3 수량형 변수의 분석

* 데이터 분포의 시각화: hist(), boxplot(), ggplot() + geom_{histogram, density}()
* 요약 통계량 계산: summary(), mean(), median(), var(), sd(), mad(), quantile()
* 데이터의 정규성 검사: qqplot(), qqline()
* 가설검정과 신뢰구간: t.test()
    + 실제 데이터의 분포가 정규분포가 아니라도 큰 문제가 되지 않음
* 이상점 찾아보기: 로버스트 통계량 계산

```{r}
summary(mpg$hwy)
mean(mpg$hwy)
median(mpg$hwy)
range(mpg$hwy)
quantile(mpg$hwy)

opar <- par(mfrow=c(2,2))
hist(mpg$hwy)
boxplot(mpg$hwy)
qqnorm(mpg$hwy)
qqline(mpg$hwy)
par(opar)
```

### 7.3.1 일변량 t-검정

```{r}
hwy <- mpg$hwy
n <- length(hwy)
mu0 <- 22.9
t.test(hwy, mu=mu0, alternative="greater")
```

* two.sided test를 이용해 95 percent 신뢰구간 추출가능

```{r}
t.test(hwy)
```

### 7.3.2 이상점과 로버스트 통계 방법

* 이상점(outliers)는 여러 이유로 발생하는 이상 관측치
* 흔한 이유는 데이터 입력 오류
* 로버스트 통계 방법은 이상점의 영향을 적게 받는 절차
    + 평균 대신 중앙값(median), 표준편차 대신 MAD(Median Absolute Deviance)

```{r}
c(mean(hwy), sd(hwy))
c(median(hwy), mad(hwy))
```

## 7.4 성공-실패값 범주형 변수의 분석

* 요약 통계량 계산: table(), xtabs()
* 데이터 분포의 시각화: barplot()
* 가설검정과 신뢰구간: binom.test()

```{r}
set.seed(1606)
n <- 100
p <- 0.5
x <- rbinom(n, 1, p)
x <- factor(x, levels=c(0, 1), labels=c("no", "yes"))
x
table(x)
prop.table(table(x))
barplot(table(x))

binom.test(x=length(x[x=='yes']), n=length(x), p=0.5, alternative="two.sided")
```

### 7.4.1 오차한계, 표본 크기, sqrt(n)의 힘

* 오차한계(margin of error): 주어진 신뢰수준에서 신뢰구간 크기의 절반
* 오차한계는 샘플 수가 커짐에 따라 sqrt(n)의 속도로 줄어듬

```{r}
binom.test(x=5400, n=10000)

n <- c(100, 1000, 2000, 10000, 1e6)
data.frame(n=n, moe=round(1.96 * sqrt(1/(4*n)), 4))

curve(1.96 * sqrt(1/(4 * x)), 10, 10000, log='x')
grid()
```

## 7.5 설명변수와 반응변수

* 설명변수(explanatory variable): 예측변수(predictor variable), 독립변수(independent variable)
* 반응변수(response variable): 종속변수(dependent variable)

## 7.6 수량형 X, 수량형 Y의 분석

1. 산점도를 통해 관계의 모양을 파악
    + plot(), ggplot2::geom_point()
    + 중복치가 많을 때는 jitter. 데이터수가 너무 많을 때는 alpha= 옵션
    + 관계가 선형인지, 강한지 약한지, 이상치가 있는지 파악
2. 상관계수를 계산
3. 선형 모형을 적합: 잔차의 분포와 이상점 파악
4. 이상치가 있을 때는 로버스트 회귀분석 사용
5. 비선형 데이터에는 LOESS 등의 평활법을 사용

### 7.6.1 산점도

```{r}
ggplot(mpg, aes(cty, hwy)) + geom_jitter() + geom_smooth(method="lm")
```

### 7.6.2 상관계수

* 피어슨 상관계수로 선형 상관관계 파악
* 산점도 없이 상관관계만 보는 것은 위험
* 로버스트 방법인 켄달 타우나 스피어만 로를 사용하는 것도 좋은 방법
```{r}
cor(mpg$cty, mpg$hwy)
with(mpg, cor(cty, hwy))
with(mpg, cor(cty, hwy, method = "kendall"))
with(mpg, cor(cty, hwy, method = "spearman"))
```

### 7.6.3 선형회귀 모형 적합

* lm(), summary.lm() 함수는 최소제곱법(least square method)로 절편, 경사값의 추정치 구함
* 각 모수값이 0인지 아닌지에 대한 가설 검정 결과 보여줌

```{r}
(hwy_lm <- lm(hwy ~ cty, data=mpg))
summary(hwy_lm)
```

### 7.6.4 모형 적합도 검정

* 적합도(goodness of fit)
    + 총 제곱합(total sum of squares, SST): 모형화 전의 반응 변수의 변동
    + 회귀 제곱합(regression sum of square, SSR): 회귀분석모형으로 설명되는 반응변수의 변동
    + 잔차 제곱합(error sum of square, SSE): 모형으로 설명되지 않는 반응변수의 변동
    + SST = SSR + SSE
    + Multiple R-squared: SSR / SST - 결정계수(coefficient of determination)라고 불림
* R-squared는 설명 변수를 모형에 추가할 때 마다 증가
* 이 영향을 통제하며 설명력을 보는 것이 Adjusted R-squared
* F-statistic은 절편 외에 다른 모수는 효과가 없다(H0) vs. H0이 아니다(H1) 에 대한 검정통계량

### 7.6.5 선형회귀 모형 예측

* predict(): 반응변수 예측값 계산
* resid(): 잔차 계산

```{r}
predict(hwy_lm)
resid(hwy_lm)
predict(hwy_lm, newdata = data.frame(cty=c(10, 20, 30)))
predict(hwy_lm, newdata = data.frame(cty=c(10, 20, 30)), se.fit=TRUE)
```

### 7.6.6 선형회귀 모형의 가정 진단

* 선형회귀 모형의 가정
    1. x와 y의 관계가 선형이다.
    2. 잔차의 분포가 독립이다.
    3. 잔차의 분포가 동일하다.
    4. 잔차의 분포가 N(0, sigma^2)이다. (중요하지 않음)
* 조건 4는 중요하지 않지만, 조건 3이 어긋날 경우 추정치와 오차의 유효성에 영향 (heteroscedastic error distribution)
    + weighted regression 기법으로 해결 가능
* 회귀분석 진단(regression diagnostic)

```{r}
class(hwy_lm)
opar <- par(mfrow=c(2,2), oma=c(0,0,1.1,0))
plot(hwy_lm, las=1)
par(opar)
```

### 7.6.7 로버스트 선형회귀분석

* 수량형 변수에 이상치 있을때는 로버스트 통계 분석
* 선형회귀분석 모수의 추정값도 분포에 이상치가 있을 경우 민감하게 반응 - 최소제곱법 활용하기 때문
* 이때 로버스트 회귀분석 사용가능: 좋은 관측치만 적합에 사용

```{r}
library(MASS)
set.seed(123)
lqs(stack.loss ~., data=stackloss)
lm(stack.loss ~., data=stackloss)
```

### 7.6.8 비선형/비모수적 방법, 평활법과 LESS

* 평활법(smoothing): 모델에 아무 가정도 하지 않음
* LOESS(locally weighted scatterplot smoothing): 국소 회귀(local regression) 방법 중 하나로 각 예측변수 값에서 가까운 k개 값의 관측지를 사용하여 2차 다항회귀 모형을 적합하여 다양한 x 값에 반복하는 것.

```{r}
plot(hwy ~ displ, data=mpg)
mpg_lo <- loess(hwy ~displ, data=mpg)
mpg_lo
summary(mpg_lo)

xs <- seq(2,7,length.out=100)
mpg_pre <- predict(mpg_lo, newdata=data.frame(displ=xs), se=TRUE)
lines(xs, mpg_pre$fit)
lines(xs, mpg_pre$fit - 1.96*mpg_pre$se.fit, lty=2)
lines(xs, mpg_pre$fit + 1.96*mpg_pre$se.fit, lty=2)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_smooth()
```

## 7.7 범주형 x, 수량형 y

1. 병렬상자그림(side-by-side boxplot)을 이용하여 데이터를 시각화
2. lm()으로 ANOVA 선형 모형 적합
3. plot.lm()으로 잔차의 분포를 살펴봄

### 7.7.1 분산분석(ANOVA)

### 7.7.2 선형 모형, t-검정의 위대함

* ANOVA는 선형 분석의 특별한 예로 수학적으로는 동일

### 7.7.3 분산분석 예

```{r}
mpg %>% ggplot(aes(class, hwy)) + geom_boxplot()

(hwy_lm2 <- lm(hwy ~class, data=mpg))
summary(hwy_lm2)

predict(hwy_lm2, neadata=data.frame(class="pickup"))
```

### 7.7.4 분산분석의 진단

1. 잔차의 분포가 독립이다.
2. 잔차의 분산이 동일하다.
3. 잔차의 분포가 N(0, sigma^2) 이다.

분포의 독립성과 이상치의 유무가 중요함.

```{r}
opar <- par(mfrow = c(2,2), oma=c(0, 0, 1.1, 0))
plot(hwy_lm2, las=1)
par(opar)
```

## 7.8 수량형 x, 범주형 y(성공-실패)

1. X와 Y변수의 산점도: Y변수 값에 따른 X변수의 분포 차이, 두 변수간 관계, 이상치 여부, 표본 로그오즈와 x의 산점도에 선형 패턴이 있는지
2. glm()함수로 일반화 선형 모형 적합: summary.glm() 함수로 심도 있는 결과
3. plot.glm()으로 잔차의 분포 조사

### 7.8.1 일반화 선형 모형, 로짓/로지스틱 함수

* 반응변수는 베르누이 확률변수
* 모수벡터의 추정값은 최대우도법(Maximum Likelihood Estimation)으로 계산
* 이항분포와 로짓 링크 함수(로지스틱 역함수)를 사용한 GLM 모형을 로지스틱 회귀 모형이라고 함

### 7.8.2 챌린저 데이터 분석

```{r}
chall <- read.csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/challenger.csv')
chall <- tbl_df(chall)
glimpse(chall)
```

```{r}
chall %>% ggplot(aes(temperature, distress_ct)) +
  geom_point()
chall %>% ggplot(aes(factor(distress_ct), temperature)) +
  geom_boxplot()
```

```{r}
(chall_glm <-
   glm(cbind(distress_ct, o_ring_ct - distress_ct) ~
         temperature, data=chall, family='binomial'))
summary(chall_glm)
```

### 7.8.3 GLM의 모형 적합도

* Null deviance: 모형을 적합하기 전의 deviance
* Residual deviance: 모형을 적합한 후의 deviance
    + 모형이 적합하지 않다는 가정 하에서 두 deviance의 차이는 chi-squared 분포를 따름
    + chisq test를 통해 p-value 파악 가능
* AIC(Akaike Information Criterion)

### 7.8.4 로지스틱 모형 예측, 링크와 반응변수

* predict.glm()을 그냥 쓰면 0과 1 사이 값으로 나오지 않음

```{r}
predict(chall_glm, data.frame(temperature=30))
```

* type="response" 옵션 이용

```{r}
exp(3.45) / (exp(3.45) + 1)
predict(chall_glm, data.frame(temperature=30), type="response")
```

### 7.8.5 로지스틱 모형 적합결과의 시각화

```{r}
logistic <- function(x){exp(x)/(exp(x)+1)}
plot(c(20,85), c(0,1), type="n", xlab="temperature",
     ylab="prob")
tp <- seq(20, 85, 1)
chall_glm_pred <-
  predict(chall_glm,
          data.frame(temperature = tp),
          se.fit=TRUE)
lines(tp, logistic(chall_glm_pred$fit))
lines(tp, logistic(chall_glm_pred$fit - 1.96 * chall_glm_pred$se.fit), lty=2)
lines(tp, logistic(chall_glm_pred$fit + 1.96 * chall_glm_pred$se.fit), lty=2)
abline(v=30, lty=2, col='blue')
```

### 7.8.6 범주형 y 변수의 범주가 셋 이상일 경우

* one vs. rest

### 7.8.7 GLM 모형의 일반화

* binomial: link="logit"
* gaussian: link="identity"
* Gamma: link="inverse"
* poisson: link="log"

## 7.9 더 복잡한 데이터의 분석, 머신러닝, 데이터 마이닝